
DATA:
  DATASET: inat21
  IMG_SIZE: 192
  NUM_WORKERS: 32
MODEL:
  TYPE: swinv2
  NAME: swinv2_base_window12
  DROP_PATH_RATE: 0.2
  SWINV2:
    EMBED_DIM: 128
    DEPTHS: [ 2, 2, 18, 2 ]
    NUM_HEADS: [ 4, 8, 16, 32 ]
    WINDOW_SIZE: 12
TRAIN:
  # Want a global batch size of 2048 because SwinV2 was trained on 16 V100s with batch size 128 (I think)
  # But we are going to use a global batch size of 1024 because it's faster (throughput).
  GLOBAL_BATCH_SIZE: 1024

  # We are using limited epochs based on pre-training configs for imagenet22k
  # Then we will pre-train on 256x256 for 30 epochs
  EPOCHS: 90
  WARMUP_EPOCHS: 5
  WEIGHT_DECAY: 0.1

  # Use 1/4 of original learning rates
  BASE_LR: 1.25e-4
  WARMUP_LR: 1.25e-7
  MIN_LR: 1.25e-6

  # Weighting of the levels of the tree (uniform, exponential) (for hot-ice and red-chilli)
  WEIGHTING: uniform  #("exponential" for hot-ice)
  # Co-efficient value for computing weights (0.1,.0.2, ..., 0.9)
  ALPHA: 0.1  
  # choose a loss function to use for training
  LOSS: fuzzy-fig # (default is fuzzy-fig) [fuzzy-fig, groovy-grape, hot-ice, red-chilli]

  HIERARCHICAL_COEFFS: [ 8, 5.65, 4, 2.82, 2, 1.41, 1 ]

  # Percentage of data for to train in a low-data regieme (default 1.0)
  DATA_PERCENTAGE: 1.0

EXPERIMENT:
  NAME: red-chilli-192
  WANDB_ID: rcal001

HIERARCHICAL: false #('false' to test fuzzy-fig loss)


